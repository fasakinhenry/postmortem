<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title></title>
    <link rel="stylesheet" href="/assets/css/style.css" />
  </head>
  <body>
    <header>
      <h1>My Cool Project</h1>
      <p>A postmortem analysis</p>
    </header>

    <main>
      <article><p><strong>Incident Summary</strong></p>

<p>Between 10:00 UTC and 12:30 UTC on 2024-09-18, approximately 85% of Consolve users encountered service disruptions, including slow response times, failed login attempts, and errors while attempting to connect with service providers. The incident was triggered by high traffic on Consolveâ€™s launch day, overwhelming the backend infrastructure. The root cause was database connection pool exhaustion, preventing the app from scaling effectively under load. The severity of the incident was classified as high, and the impact lasted for 2 hours and 30 minutes.</p>

<p>The event was detected by automated monitoring systems, which alerted the engineering team. Immediate actions were taken to restore service by scaling infrastructure and optimizing database performance. During the outage, over 150 support tickets were submitted, along with a spike in social media mentions.</p>

<hr />

<p><strong>Leadup</strong></p>

<p>On 2024-09-18 at 09:45 UTC, Consolve officially launched. The team had anticipated increased traffic but underestimated the volume of concurrent users. The backend infrastructure was scaled based on pre-launch projections but was not stress-tested to match the traffic levels encountered.</p>

<p>At 10:00 UTC, over 10,000 requests per minute were sent to the backend, which triggered an overload in the system, leading to database connection pool exhaustion and service degradation. Initial attempts to handle the traffic increase failed, as the server was unable to allocate enough resources in time to accommodate the spike.</p>

<hr />

<p><strong>Fault</strong></p>

<p>The backend connection pool was configured with a limit too low to handle the surge in requests, causing errors when attempting to establish new database connections. The web app returned 503 errors to users trying to log in or interact with service providers. This continued for approximately 2 hours until corrective action was taken. The lack of real-time monitoring of database performance metrics contributed to the delay in identifying the connection pool issue.</p>

<hr />

<p><strong>Impact</strong></p>

<p>For 2 hours and 30 minutes between 10:00 UTC and 12:30 UTC on 2024-09-18, Consolve users experienced widespread disruption. 85% of all users (approximately 30,000) were unable to access key services, resulting in failed logins, slow page loads, and connection errors when attempting to interact with service providers. During this time, over 150 support tickets were submitted, and the issue was widely reported on social media, resulting in over 300 mentions of the outage.</p>

<hr />

<p><strong>Detection</strong></p>

<p>The incident was detected at 10:05 UTC when the automated monitoring system triggered alerts for high latency and failed requests. The alert was sent to the on-call engineering team, who immediately began investigating the cause. However, there was a delay of 20 minutes in identifying the root cause due to insufficient real-time monitoring for database connection limits. To improve future detection, enhanced database monitoring with more granular alerts will be implemented.</p>

<hr />

<p><strong>Response</strong></p>

<p>The on-call engineer received the alert at 10:05 UTC and started investigating by checking the server logs and application performance. The initial assumption was that the application servers were underperforming, and actions were taken to increase their capacity. At 10:45 UTC, the incident was escalated to the database administration team after noticing a bottleneck in connection handling. By 11:30 UTC, the database connection pool size was increased, and additional read replicas were provisioned to distribute the load. Service was fully restored by 12:30 UTC.</p>

<hr />

<p><strong>Recovery</strong></p>

<p>The service was restored by increasing the database connection pool size and adding more read replicas to offload the demand from the primary database instance. Additionally, auto-scaling was configured for the backend servers to dynamically adjust capacity during traffic spikes. Database performance was continuously monitored, and by 12:30 UTC, all users were able to access the platform without issues. To reduce recovery time in the future, stress tests and automated scaling processes will be improved.</p>

<hr />

<p><strong>Timeline</strong></p>

<ul>
  <li>10:00 UTC - Consolve launched; initial surge of traffic begins.</li>
  <li>10:05 UTC - Automated monitoring system alerts for high latency and failed requests.</li>
  <li>10:15 UTC - Initial investigation begins, assuming server performance issues.</li>
  <li>10:45 UTC - Escalation to database team due to connection pool exhaustion.</li>
  <li>11:30 UTC - Database connection pool increased; read replicas added.</li>
  <li>12:30 UTC - Full recovery; system restored, traffic normalized.</li>
</ul>

<hr />

<p><strong>Root Cause Identification: The Five Whys</strong></p>

<ol>
  <li>The Consolve app had an outage because the database connection pool was exhausted.</li>
  <li>The database connection pool was exhausted because the traffic exceeded pre-launch projections.</li>
  <li>The traffic exceeded projections because the app was not adequately stress-tested for high user volumes.</li>
  <li>The app was not stress-tested because the team underestimated peak traffic and relied on outdated traffic models.</li>
  <li>Traffic models were outdated because real-time user behavior during launch was not incorporated into the projections.</li>
</ol>

<hr />

<p><strong>Root Cause</strong></p>

<p>The root cause of the incident was an under-configured database connection pool, combined with insufficient load-testing prior to launch. The app was unable to handle the number of concurrent requests, leading to degraded performance and eventual service unavailability.</p>

<hr />

<p><strong>Backlog Check</strong></p>

<p>A review of the backlog indicated that while there were discussions around scaling, stress tests for the database connection pool size were not prioritized. Load testing and stress simulations were suggested but deprioritized due to time constraints leading up to launch.</p>

<hr />

<p><strong>Recurrence</strong></p>

<p>This issue has not been previously observed at this scale, though smaller incidents related to connection pool limits have been logged in the past (INC-0021, INC-0087). The lack of proactive scaling led to this recurrence on a larger scale during launch day.</p>

<hr />

<p><strong>Lessons Learned</strong></p>

<ul>
  <li>Improved load testing is necessary to better anticipate user behavior under peak conditions.</li>
  <li>Database connection pool settings should be dynamically adjusted based on traffic.</li>
  <li>Real-time monitoring for database performance metrics would have reduced detection time.</li>
  <li>Stress testing for peak traffic should be scheduled ahead of major launches.</li>
</ul>

<hr />

<p><strong>Corrective Actions</strong></p>

<ul>
  <li>Increase the default size of the database connection pool and enable auto-scaling for traffic spikes.</li>
  <li>Add real-time database performance monitoring with more granular alerts to detect future bottlenecks.</li>
  <li>Conduct full-scale load testing ahead of any major releases or launches.</li>
  <li>Review and update traffic projections regularly, especially in the lead-up to launches.</li>
</ul>

<p><strong>Owners</strong>: Backend Team and DevOps<br />
<strong>Due Date</strong>: 2024-09-30</p>
</article>
    </main>

    <footer>
      <p>
        Postmortem Template by
        <a href="https://github.com/fasakinhenry">Fasakin Henry</a>
      </p>
    </footer>
  </body>
</html>
